{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X\n",
    "# dependencies\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Image\n",
    "from sys import platform\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.example.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X\n",
    "# import Splinter and set the chromedriver path\n",
    "from splinter import Browser\n",
    "# Setup splinter\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "# This has run successfully when you see \"Driver has been saved in cache\" in pink below\n",
    "# and a new browser window opens with nothing in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step One: the scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X\n",
    "# scraping NASA Mars News\n",
    "# collect the latest News Title and Paragraph Text from redplanetscience.com...\n",
    "url = \"https://redplanetscience.com\"\n",
    "browser.visit(url)\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve page with the requests module\n",
    "response = requests.get(url)\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O\n",
    "# ...and save them to variables for use later.\n",
    "\n",
    "# <div class=\"content_title\">NASA's Mars Reconnaissance Orbiter Undergoes Memory Update</div>\n",
    "# div.content_title seems to be the latest news title.\n",
    "news_title_results = soup.find_all('div', class_='content_title')\n",
    "news_title_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <div class=\"article_teaser_body\">...</div>\n",
    "# div.article_teaser_body seems to be the paragraph text from the latest news title.\n",
    "news_graf_results = soup.find_all('div', class_='article_teaser_body')\n",
    "news_graf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X\n",
    "# scraping JPL Mars Space Images - Featured Image\n",
    "# visit spaceimages-mars.com...\n",
    "url = \"https://spaceimages-mars.com\"\n",
    "browser.visit(url)\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X\n",
    "# ...and use Splinter to navigate the site & find the URL of the current Featured Mars Image \n",
    "# and assign the url string to a variable called featured_image_url.\n",
    "# Find the URL of the *full size* .jpg image.\n",
    "# Save a complete url string for this image, e.g.\n",
    "# featured_image_url = 'https://spaceimages-mars.com/image/featured/mars2.jpg'\n",
    "\n",
    "# The current featured image seems to be\n",
    "# <img class=\"headerimage fade-in\" src=\"image/featured/mars3.jpg\">\n",
    "# featured_image_relative_url = somehow get what's in src out of the above\n",
    "featured_image_relative_url = \"image/featured/mars3.jpg\"\n",
    "featured_image_url = f\"{domain}{featured_image_relative_url}\"\n",
    "print(featured_image_url)\n",
    "Image(featured_image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpath = '//td//a[@class=\"list_image\"]/img'\n",
    "\n",
    "# Use splinter to Click the \"Mars in natural color in 2007\" image \n",
    "# to bring up the full resolution image\n",
    "results = browser.find_by_xpath(xpath)\n",
    "img = results[0]\n",
    "img.click()\n",
    "\n",
    "# Scrape the browser into soup and use soup to find the full resolution image of mars\n",
    "# Save the image url to a variable called `img_url`\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "img_url = soup.find(\"img\", class_=\"jpg\")[\"src\"]\n",
    "img_url\n",
    "Image(url='img.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X\n",
    "# scraping Mars Facts\n",
    "# visit galaxyfacts-mars.com\n",
    "url = \"https://galaxyfacts-mars.com\"\n",
    "browser.visit(url)\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X\n",
    "# and use Pandas to scrape the table containing facts about the planet including:\n",
    "# diameter, mass, usw.\n",
    "\n",
    "# <table class=\"table table-striped\"> seems to be this table.\n",
    "HTML_tables = pd.read_html(url)\n",
    "# HTML_tables\n",
    "# type(HTML_tables)\n",
    "df_from_html_table = HTML_tables[0]\n",
    "df_from_html_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X\n",
    "# Use Pandas to convert the data to a HTML table string.\n",
    "# Something about scraping the Wikipedia entry for state capitols.\n",
    "# url = 'https://en.wikipedia.org/wiki/List_of_capitals_in_the_United_States'\n",
    "html_from_df = df_from_html_table.to_html()\n",
    "df_from_html_table.to_html('table.html')\n",
    "print(html_from_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X\n",
    "# If you're running macOS, this opens the table in its own browser window\n",
    "if platform == \"darwin\":\n",
    "    !open table.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X\n",
    "# scraping Mars Hemispheres\n",
    "# Visit marshemispheres.com...\n",
    "url = \"https://marshemispheres.com\"\n",
    "browser.visit(url)\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X\n",
    "# ...to obtain high resolution images for each of Mars's hemispheres.\n",
    "# Click each of the links to the hemispheres to find the URL of the full resolution image.\n",
    "# Save both the image url string for the full resolution hemisphere image,\n",
    "# and the Hemisphere title containing the hemisphere name.\n",
    "# Use a Python dictionary to store the data using the keys img_url and title.\n",
    "# Append the dictionary with the image url string and the hemisphere title to a list.\n",
    "# This list will contain one dictionary for each hemisphere.\n",
    "\n",
    "hemisphere_image_urls = [\n",
    "    {\"title\": \"Cerberus Hemisphere\", \"img_url\": \"https://marshemispheres.com/images/cerberus_enhanced.tif\", \"extent\":\"The Cerberus image is centered at 3 lat, 185 long.\"},\n",
    "    {\"title\": \"Schiaparelli Hemisphere\", \"img_url\": \"https://marshemispheres.com/images/schiaparelli_enhanced.tif\", \"extent\":\"The Schiaparelli image extends from -60 to 60 lat and from 260 to 30 long.\"},\n",
    "    {\"title\": \"Syrtis Major Hemisphere\", \"img_url\": \"https://marshemispheres.com/images/syrtis_major_enhanced.tif\", \"extent\":\"The Syrtis Major image extends from -60 to 60 lat and from 260 to 350 long.\"},\n",
    "    {\"title\": \"Valles Marineris Hemisphere\", \"img_url\": \"https://marshemispheres.com/images/valles_marineris_enhanced.tif\", \"extent\":\"The Valles Marineris image is centered at -8 lat, 78 long.\"}\n",
    "]\n",
    "hemisphere_image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X\n",
    "# close the session\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: MongoDB and Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MongoDB with Flask templating to create a new HTML page\n",
    "# that displays all of the information that was scraped from the URLs above.\n",
    "\n",
    "# Start by converting your Jupyter notebook into a Python script called scrape_mars.py\n",
    "# with a function called scrape that will execute all of your scraping code from above\n",
    "# and return one Python dictionary containing all of the scraped data.\n",
    "\n",
    "# Next, create a route called /scrape that will import your scrape_mars.py script and\n",
    "# call your scrape function.\n",
    "\n",
    "# Store the return value in Mongo as a Python dictionary.\n",
    "\n",
    "# Create a root route / that will query your Mongo database and\n",
    "# pass the mars data into an HTML template to display the data.\n",
    "\n",
    "# Create a template HTML file called index.html that will take the mars data dictionary and\n",
    "# display all of the data in the appropriate HTML elements.\n",
    "# Use the following as a guide for what the final product should look like,\n",
    "# but feel free to create your own design:\n",
    "print(\"https://umn.bootcampcontent.com/University-of-Minnesota-Boot-Camp/uofm-stp-data-pt-12-2020-u-c/tree/master/02-Homework/12-Web-Scraping-and-Document-Databases/Instructions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the Jupyter Notebook containing the scraping code and\n",
    "# screenshots of your final application\n",
    "# to your GitHub repository and submit the link.\n",
    "# Ensure your repository has regular commits (i.e. 20+ commits) and\n",
    "# a thorough README.md file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Splinter to navigate the sites when needed and\n",
    "# BeautifulSoup to help find and parse out the necessary data.\n",
    "\n",
    "# Use Pymongo for CRUD applications for your database.\n",
    "# For this homework, you can simply overwrite the existing document each time\n",
    "# the /scrape url is visited and new data is obtained.\n",
    "\n",
    "# Use Bootstrap to structure your HTML template."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
