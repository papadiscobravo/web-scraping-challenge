{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Image\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 89.0.4389\n",
      "[WDM] - Get LATEST driver version for 89.0.4389\n",
      "[WDM] - Driver [/Users/pbernhardt/.wdm/drivers/chromedriver/mac64/89.0.4389.23/chromedriver] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import Splinter and set the chromedriver path\n",
    "from splinter import Browser\n",
    "# Setup splinter\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step One: the scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping NASA Mars News\n",
    "# collect the latest News Title and Paragraph Text from redplanetscience.com...\n",
    "url = \"https://redplanetscience.com\"\n",
    "browser.visit(url)\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...and save them to variables for use later.\n",
    "\n",
    "# <div class=\"content_title\">NASA's Mars Reconnaissance Orbiter Undergoes Memory Update</div>\n",
    "# div.content_title seems to be the latest news title.\n",
    "\n",
    "# <div class=\"article_teaser_body\">...</div>\n",
    "# div.article_teaser_body seems to be the paragraph text from the latest news title.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping JPL Mars Space Images - Featured Image\n",
    "# visit spaceimages-mars.com...\n",
    "domain = \"https://spaceimages-mars.com/\"\n",
    "print(domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...and use Splinter to navigate the site & find the URL of the current Featured Mars Image \n",
    "# and assign the url string to a variable called featured_image_url.\n",
    "# Find the URL of the *full size* .jpg image.\n",
    "# Save a complete url string for this image, e.g.\n",
    "# featured_image_url = 'https://spaceimages-mars.com/image/featured/mars2.jpg'\n",
    "\n",
    "# The current featured image seems to be\n",
    "# <img class=\"headerimage fade-in\" src=\"image/featured/mars3.jpg\">\n",
    "# featured_image_relative_url = somehow get what's in src out of the above\n",
    "featured_image_relative_url = \"image/featured/mars3.jpg\"\n",
    "featured_image_url = f\"{domain}{featured_image_relative_url}\"\n",
    "print(featured_image_url)\n",
    "Image(featured_image_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpath = '//td//a[@class=\"list_image\"]/img'\n",
    "\n",
    "# Use splinter to Click the \"Mars in natural color in 2007\" image \n",
    "# to bring up the full resolution image\n",
    "results = browser.find_by_xpath(xpath)\n",
    "img = results[0]\n",
    "img.click()\n",
    "\n",
    "# Scrape the browser into soup and use soup to find the full resolution image of mars\n",
    "# Save the image url to a variable called `img_url`\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "img_url = soup.find(\"img\", class_=\"jpg\")[\"src\"]\n",
    "img_url\n",
    "Image(url='img.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping Mars Facts\n",
    "# visit galaxyfacts-mars.com\n",
    "print(\"https://galaxyfacts-mars.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and use Pandas to scrape the table containing facts about the planet including:\n",
    "# diameter, mass, usw.\n",
    "\n",
    "# <table class=\"table table-striped\"> seems to be this table.\n",
    "\n",
    "# Use Pandas to convert the data to a HTML table string.\n",
    "# Something about scraping the Wikipedia entry for state capitols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping Mars Hemispheres\n",
    "# Visit marshemispheres.com...\n",
    "print(\"https://marshemispheres.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...to obtain high resolution images for each of Mars's hemispheres.\n",
    "# Click each of the links to the hemispheres to find the URL of the full resolution image.\n",
    "# Save both the image url string for the full resolution hemisphere image,\n",
    "# and the Hemisphere title containing the hemisphere name.\n",
    "# Use a Python dictionary to store the data using the keys img_url and title.\n",
    "# Append the dictionary with the image url string and the hemisphere title to a list.\n",
    "# This list will contain one dictionary for each hemisphere.\n",
    "\n",
    "# Example:\n",
    "# hemisphere_image_urls = [\n",
    "#    {\"title\": \"Valles Marineris Hemisphere\", \"img_url\": \"...\"},\n",
    "#    {\"title\": \"Cerberus Hemisphere\", \"img_url\": \"...\"},\n",
    "#    {\"title\": \"Schiaparelli Hemisphere\", \"img_url\": \"...\"},\n",
    "#    {\"title\": \"Syrtis Major Hemisphere\", \"img_url\": \"...\"},\n",
    "# ]\n",
    "\n",
    "hemisphere_image_urls = []\n",
    "hemisphere_image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the session\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: MongoDB and Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MongoDB with Flask templating to create a new HTML page\n",
    "# that displays all of the information that was scraped from the URLs above.\n",
    "\n",
    "# Start by converting your Jupyter notebook into a Python script called scrape_mars.py\n",
    "# with a function called scrape that will execute all of your scraping code from above\n",
    "# and return one Python dictionary containing all of the scraped data.\n",
    "\n",
    "# Next, create a route called /scrape that will import your scrape_mars.py script and\n",
    "# call your scrape function.\n",
    "\n",
    "# Store the return value in Mongo as a Python dictionary.\n",
    "\n",
    "# Create a root route / that will query your Mongo database and\n",
    "# pass the mars data into an HTML template to display the data.\n",
    "\n",
    "# Create a template HTML file called index.html that will take the mars data dictionary and\n",
    "# display all of the data in the appropriate HTML elements.\n",
    "# Use the following as a guide for what the final product should look like,\n",
    "# but feel free to create your own design:\n",
    "print(\"https://umn.bootcampcontent.com/University-of-Minnesota-Boot-Camp/uofm-stp-data-pt-12-2020-u-c/tree/master/02-Homework/12-Web-Scraping-and-Document-Databases/Instructions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the Jupyter Notebook containing the scraping code and\n",
    "# screenshots of your final application\n",
    "# to your GitHub repository and submit the link.\n",
    "# Ensure your repository has regular commits (i.e. 20+ commits) and\n",
    "# a thorough README.md file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Splinter to navigate the sites when needed and\n",
    "# BeautifulSoup to help find and parse out the necessary data.\n",
    "\n",
    "# Use Pymongo for CRUD applications for your database.\n",
    "# For this homework, you can simply overwrite the existing document each time\n",
    "# the /scrape url is visited and new data is obtained.\n",
    "\n",
    "# Use Bootstrap to structure your HTML template."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
